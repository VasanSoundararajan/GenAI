{"cells":[{"cell_type":"markdown","metadata":{"id":"ylRyMRnBtNjL"},"source":["# __Demo: Implementing a Variational Autoencoder (VAE) with TensorFlow for Image Generation Using the MNIST Dataset__"]},{"cell_type":"markdown","metadata":{"id":"ynqG61OBXVWD"},"source":["# __Steps to Perform__"]},{"cell_type":"markdown","metadata":{"id":"17rpQjjgtnOu"},"source":["Step 1: Import the Necessary Libraries\n","\n","Step 2: Load the MNIST Dataset\n","\n","Step 3: Set Hyperparameters\n","\n","Step 4: Define Model Architecture\n","\n","Step 5: Define the Sampling Function\n","\n","Step 6: Connect the Encoder and Decoder\n","\n","Step 7: Define the Loss Function and Compile the Model\n","\n","Step 8: Train the Model\n","\n","Step 9: Generate a Manifold of Digits"]},{"cell_type":"markdown","metadata":{"id":"JWmOWWvhEsiq"},"source":["# __Step 1: Import the Necessary Libraries__\n","- Import numpy, matplotlib.pyplot, and tensorflow."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"OKSPmDg6EL4s","outputId":"9c7b9755-ebfc-4518-f652-2fe9f8b8e46b"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf # pip install tensorflow"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["'2.15.0'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tf.__version__"]},{"cell_type":"markdown","metadata":{"id":"aDFCTWYpFPXH"},"source":["# __Step 2: Load the MNIST Dataset__"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"8XGexe6vFfi_"},"outputs":[],"source":["mnist = tf.keras.datasets.mnist\n","(x_train, _), (x_test, _) = mnist.load_data() # not interested in the labels\n","x_train, x_test = x_train / 255.0, x_test / 255.0 # scale between 0 and 1"]},{"cell_type":"markdown","metadata":{"id":"aV8L1xunFz4c"},"source":["# __Step 3: Set Hyperparameters__\n","-  Define the `learning_rate`, `num_steps`, and `batch_size`.\n","- `learning_rate` is the step size at each iteration while moving toward a minimum of a loss function.\n","- `num_steps` is the number of steps you want to train the model.\n","- `batch_size` is the number of samples that will be propagated through the network."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_5G6K-QMF2yx"},"outputs":[],"source":["learning_rate = 0.001\n","num_steps = 100\n","batch_size = 64 # default 32"]},{"cell_type":"markdown","metadata":{"id":"ulE606vVlTdt"},"source":["- In the next steps, you will define the architecture of the VAE, including the encoder and decoder networks and the loss function."]},{"cell_type":"markdown","metadata":{"id":"v1L3udT1lbL-"},"source":["# __Step 4: Define Model Architecture__\n","- Construct the VAE using `tf.keras` that defines the encoder and decoder using keras layers."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Y7GGMERkXVWJ","outputId":"441eb5be-66a9-4fc4-a022-61863e024285"},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-02-23 13:08:13.295523: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n","2025-02-23 13:08:13.295914: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n","2025-02-23 13:08:13.296519: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n","2025-02-23 13:08:13.297155: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n","2025-02-23 13:08:13.297618: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"]}],"source":["latent_dim = 2 # Example latent space dimension. can be anything lower than original image dim\n","# try with a higher value for latent_dim = for eg 256\n","\n","# Define the encoder part\n","encoder_inputs = tf.keras.Input(shape=(28, 28))\n","x = tf.keras.layers.Flatten()(encoder_inputs)\n","x = tf.keras.layers.Dense(512, activation='relu')(x) # also try adding a few more leyers with progressively smaller dimension eg 784-512-384-256\n","z_mean = tf.keras.layers.Dense(latent_dim)(x)\n","z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n","\n","# Define the decoder part\n","latent_inputs = tf.keras.Input(shape=(latent_dim,))\n","x = tf.keras.layers.Dense(512, activation='relu')(latent_inputs) # 256-384-512-784 - reshape to 28x28\n","x = tf.keras.layers.Dense(784, activation='sigmoid')(x)\n","decoder_outputs = tf.keras.layers.Reshape((28, 28))(x)"]},{"cell_type":"markdown","metadata":{"id":"o6IKbLolnHBV"},"source":["# __Step 5: Define the Sampling Function__\n","\n","-  Create a custom Keras layer for the sampling function used in the VAE."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"wXeUCRb5oDgi"},"outputs":[],"source":["class Sampling(tf.keras.layers.Layer):\n","    \"\"\"\n","    Custom layer that implements the reparameterization trick for Variational Autoencoders (VAE).\n","    This layer samples points from the latent space distribution defined by z_mean and z_log_var,\n","    using the reparameterization trick to allow backpropagation through the sampling process.\n","    The trick works by separating the random sampling into a deterministic function of random input,\n","    enabling gradient flow while maintaining stochastic behavior.\n","    \n","    Inputs:\n","        - z_mean: Mean vector of the latent distribution\n","        - z_log_var: Log variance vector of the latent distribution\n","    \n","    Returns:\n","        - A sampled point from the latent distribution that can be backpropagated through\n","    \"\"\"\n","    def call(self, inputs):\n","        # inputs is a tuple of (z_mean, z_log_var) representing the parameters of the latent space distribution\n","        z_mean, z_log_var = inputs\n","        \n","        # Get the batch size and latent dimension from z_mean tensor shape\n","        batch = tf.shape(z_mean)[0]  # Number of samples in the batch\n","        dim = tf.shape(z_mean)[1]    # Dimension of the latent space\n","        \n","        # Generate random samples from a standard normal distribution (mean=0, std=1)\n","        # These act as our source of randomness for the sampling process\n","        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n","        \n","        # Implement the reparameterization trick: z = μ + σ * ε\n","        # where: \n","        # - z_mean (μ) is the mean of the latent distribution\n","        # - exp(0.5 * z_log_var) is the standard deviation σ (we use exp because z_log_var = log(σ²))\n","        # - epsilon (ε) is the random noise from standard normal distribution\n","        # This trick allows backpropagation through the random sampling process\n","        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n","        \n","    def call(self, inputs):\n","        # inputs is a tuple of (z_mean, z_log_var) representing the parameters of the latent space distribution\n","        z_mean, z_log_var = inputs\n","        \n","        # Get the batch size and latent dimension from z_mean tensor shape\n","        batch = tf.shape(z_mean)[0]  # Number of samples in the batch\n","        dim = tf.shape(z_mean)[1]    # Dimension of the latent space\n","        \n","        # Generate random samples from a standard normal distribution (mean=0, std=1)\n","        # These act as our source of randomness for the sampling process\n","        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n","        \n","        # Implement the reparameterization trick: z = μ + σ * ε\n","        # where: \n","        # - z_mean (μ) is the mean of the latent distribution\n","        # - exp(0.5 * z_log_var) is the standard deviation σ (we use exp because z_log_var = log(σ²))\n","        # - epsilon (ε) is the random noise from standard normal distribution\n","        # This trick allows backpropagation through the random sampling process\n","        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"]},{"cell_type":"markdown","metadata":{"id":"DGHzqkjloHjf"},"source":["# __Step 6: Connect the Encoder and the Decoder__\n","\n","- Use keras functional API to connect the encoder and decoder parts of the VAE."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"1gikY-lHph-Z"},"outputs":[],"source":["# Sample points from the latent space using the Sampling layer\n","# Takes z_mean and z_log_var as inputs and returns sampled points using the reparameterization trick\n","encoder_outputs = Sampling()([z_mean, z_log_var])\n","\n","# Create the encoder model with:\n","# - Input: Original images (encoder_inputs)\n","# - Outputs: [z_mean, z_log_var, sampled_points]\n","# This gives us access to both the distribution parameters and the sampled points\n","encoder = tf.keras.Model(inputs=encoder_inputs, outputs=[z_mean, z_log_var, encoder_outputs])\n","\n","# Create the decoder model with:\n","# - Input: Points from latent space (latent_inputs)\n","# - Output: Reconstructed images (decoder_outputs)\n","decoder = tf.keras.Model(inputs=latent_inputs, outputs=decoder_outputs)\n","\n","# Connect encoder and decoder to create the full VAE:\n","# 1. encoder(encoder_inputs) returns [z_mean, z_log_var, sampled_points]\n","# 2. [2] selects the sampled_points (third output)\n","# 3. decoder takes these sampled points and reconstructs the image\n","vae_outputs = decoder(encoder(encoder_inputs)[2])\n","\n","# Create the complete VAE model:\n","# - Input: Original images (encoder_inputs)\n","# - Output: Reconstructed images (vae_outputs)\n","# This is the full autoencoder that will be trained end-to-end\n","vae = tf.keras.Model(inputs=encoder_inputs, outputs=vae_outputs)"]},{"cell_type":"markdown","metadata":{"id":"AkmD7R1vprHa"},"source":["- In the next steps, you will define the loss function and train the model. After training, you can use the model to generate new images."]},{"cell_type":"markdown","metadata":{"id":"B7e13kmfpu22"},"source":["# __Step 7: Define the Loss Function and Compile the Model__\n","- The loss function in VAE typically includes a reconstruction loss and a KL divergence loss."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"o5rABU89qSjM"},"outputs":[],"source":["# Define the VAE loss within the VAE model class\n","class VAE(tf.keras.Model):\n","    \"\"\"\n","    Variational Autoencoder (VAE) implementation that combines encoder and decoder networks.\n","    \n","    The VAE learns to encode data into a probability distribution (rather than a fixed vector)\n","    and decode samples from this distribution back to the original data space.\n","    \n","    The model is trained by minimizing two losses:\n","    1. Reconstruction loss: How well the decoded samples match the original input\n","    2. KL divergence loss: How close the learned distribution is to a standard normal distribution\n","    \n","    Args:\n","        encoder: The encoder model that converts input to latent space parameters\n","        decoder: The decoder model that reconstructs input from latent space samples\n","    \"\"\"\n","    def __init__(self, encoder, decoder, **kwargs):\n","        super(VAE, self).__init__(**kwargs)\n","        self.encoder = encoder  # Store encoder model\n","        self.decoder = decoder  # Store decoder model\n","\n","    def compute_loss(self, x):\n","        \"\"\"\n","        Compute the VAE loss function which consists of reconstruction loss and KL divergence.\n","        \n","        Args:\n","            x: Input data to be reconstructed\n","            \n","        Returns:\n","            Total loss (reconstruction_loss + kl_loss)\n","        \"\"\"\n","        # Get the latent space parameters and sampled point\n","        z_mean, z_log_var, z = self.encoder(x)\n","        \n","        # Reconstruct the input using the sampled point\n","        reconstructed = self.decoder(z)\n","        \n","        # Calculate reconstruction loss (binary cross-entropy)\n","        # This measures how well the decoder reconstructs the input\n","        reconstruction_loss = tf.reduce_mean(\n","            tf.keras.losses.binary_crossentropy(x, reconstructed)\n","        )\n","        \n","        # Scale the loss by image dimensions (28x28) to maintain similar magnitude as KL loss\n","        # Without scaling: The reconstruction loss (eg.≈0.01) would be too small compared to KL loss (≈0.5)\n","        # with scaling: both reconstruction loss and KL loss are comparable. This ensures that \n","        # both objectives (good reconstruction AND proper latent space distribution) are balanced\n","\n","        reconstruction_loss *= 28 * 28\n","        \n","        # Calculate KL divergence loss\n","        # This measures how much the learned distribution differs from the prior (standard normal)\n","        # Formula: KL(N(μ, σ) || N(0, 1)) = -0.5 * sum(1 + log(σ²) - μ² - σ²)\n","        kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n","        kl_loss = tf.reduce_mean(kl_loss)  # Average over all dimensions\n","        kl_loss *= -0.5  # Scale factor from the KL divergence formula\n","        \n","        # Return total loss (sum of reconstruction and KL divergence losses)\n","        return reconstruction_loss + kl_loss\n","\n","    def train_step(self, data):\n","        \"\"\"\n","        Perform one training step on a batch of data.\n","        \n","        Args:\n","            data: A batch of training data, possibly wrapped in a tuple\n","            \n","        Returns:\n","            Dictionary containing the training loss\n","        \"\"\"\n","        # Handle the case where data comes as a tuple (e.g., with labels we don't need)\n","        if isinstance(data, tuple):\n","            data = data[0]\n","        \n","        # Use gradient tape to track operations for automatic differentiation\n","        with tf.GradientTape() as tape:\n","            # Compute the loss for this batch\n","            loss = self.compute_loss(data)\n","        \n","        # Compute gradients of the loss with respect to trainable variables\n","        grads = tape.gradient(loss, self.trainable_variables)\n","        \n","        # Apply the gradients to update model weights\n","        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n","        \n","        # Return loss metric for logging\n","        return {'loss': loss}\n","\n","# Create VAE instance by combining the pre-defined encoder and decoder\n","vae = VAE(encoder, decoder)\n","\n","# Compile the model with Adam optimizer\n","# No need to specify loss function as it's custom-defined in the model\n","vae.compile(optimizer='adam')"]},{"cell_type":"markdown","metadata":{"id":"HL4dkg-UqyoL"},"source":["# __Step 8: Train the Model__"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"SYoIni-2rJrX","outputId":"0c55fd9f-9819-4ab9-b00c-220b8dacf2de"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n"]},{"name":"stderr","output_type":"stream","text":["2025-02-23 13:21:49.822981: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"]},{"name":"stdout","output_type":"stream","text":["938/938 [==============================] - 15s 12ms/step - loss: 5186.8795\n","Epoch 2/100\n","938/938 [==============================] - 10s 11ms/step - loss: 1173.6249\n","Epoch 3/100\n","938/938 [==============================] - 10s 11ms/step - loss: 6617.2742\n","Epoch 4/100\n","938/938 [==============================] - 10s 11ms/step - loss: 7154.6378\n","Epoch 5/100\n","938/938 [==============================] - 10s 11ms/step - loss: 2490.8626\n","Epoch 6/100\n","938/938 [==============================] - 11s 11ms/step - loss: 7070.5504\n","Epoch 7/100\n","938/938 [==============================] - 10s 10ms/step - loss: 4868.0782\n","Epoch 8/100\n","938/938 [==============================] - 10s 10ms/step - loss: 2764.3214\n","Epoch 9/100\n","938/938 [==============================] - 10s 10ms/step - loss: 5861.1434\n","Epoch 10/100\n","938/938 [==============================] - 10s 11ms/step - loss: 2897.2745\n","Epoch 11/100\n","938/938 [==============================] - 10s 11ms/step - loss: 2486.6347\n","Epoch 12/100\n","938/938 [==============================] - 10s 11ms/step - loss: 3188.0891\n","Epoch 13/100\n","938/938 [==============================] - 10s 11ms/step - loss: 3047.1254\n","Epoch 14/100\n","938/938 [==============================] - 11s 11ms/step - loss: 5737.6940\n","Epoch 15/100\n","938/938 [==============================] - 10s 10ms/step - loss: 6213.0560\n","Epoch 16/100\n","938/938 [==============================] - 9s 10ms/step - loss: 6094.3484\n","Epoch 17/100\n","938/938 [==============================] - 9s 10ms/step - loss: 2920.1945\n","Epoch 18/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1858303.9834\n","Epoch 19/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3159.3348\n","Epoch 20/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1718.2685\n","Epoch 21/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1603.1846\n","Epoch 22/100\n","938/938 [==============================] - 9s 10ms/step - loss: 49348.3230\n","Epoch 23/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5899.3945\n","Epoch 24/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5816.2753\n","Epoch 25/100\n","938/938 [==============================] - 10s 10ms/step - loss: 3943.2949\n","Epoch 26/100\n","938/938 [==============================] - 10s 10ms/step - loss: 2109.5979\n","Epoch 27/100\n","938/938 [==============================] - 9s 10ms/step - loss: 4094.1711\n","Epoch 28/100\n","938/938 [==============================] - 9s 10ms/step - loss: 4845.7589\n","Epoch 29/100\n","938/938 [==============================] - 9s 9ms/step - loss: 3051.4327\n","Epoch 30/100\n","938/938 [==============================] - 9s 9ms/step - loss: 6137.4971\n","Epoch 31/100\n","938/938 [==============================] - 9s 9ms/step - loss: 6936.3511\n","Epoch 32/100\n","938/938 [==============================] - 11s 11ms/step - loss: 7137.2531\n","Epoch 33/100\n","938/938 [==============================] - 11s 12ms/step - loss: 8325.7420\n","Epoch 34/100\n","938/938 [==============================] - 10s 11ms/step - loss: 5828.5342\n","Epoch 35/100\n","938/938 [==============================] - 10s 11ms/step - loss: 6012.7558\n","Epoch 36/100\n","938/938 [==============================] - 11s 11ms/step - loss: 6122.2841\n","Epoch 37/100\n","938/938 [==============================] - 11s 11ms/step - loss: 5438.1250\n","Epoch 38/100\n","938/938 [==============================] - 11s 12ms/step - loss: 4046.0762\n","Epoch 39/100\n","938/938 [==============================] - 10s 10ms/step - loss: 2765.3347\n","Epoch 40/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1903.2529\n","Epoch 41/100\n","938/938 [==============================] - 9s 9ms/step - loss: 1705.6223\n","Epoch 42/100\n","938/938 [==============================] - 9s 9ms/step - loss: 1650.3013\n","Epoch 43/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1621.5652\n","Epoch 44/100\n","938/938 [==============================] - 10s 11ms/step - loss: 1621.4630\n","Epoch 45/100\n","938/938 [==============================] - 10s 10ms/step - loss: 1675.8774\n","Epoch 46/100\n","938/938 [==============================] - 9s 10ms/step - loss: 2587.6756\n","Epoch 47/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1683.2940\n","Epoch 48/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1639.7604\n","Epoch 49/100\n","938/938 [==============================] - 10s 10ms/step - loss: 1627.7511\n","Epoch 50/100\n","938/938 [==============================] - 9s 10ms/step - loss: 6731023155.3724\n","Epoch 51/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1627.2551\n","Epoch 52/100\n","938/938 [==============================] - 9s 9ms/step - loss: 1634.4958\n","Epoch 53/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1617.7898\n","Epoch 54/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1667.7544\n","Epoch 55/100\n","938/938 [==============================] - 9s 10ms/step - loss: 94226.8189\n","Epoch 56/100\n","938/938 [==============================] - 9s 10ms/step - loss: 1619.2609\n","Epoch 57/100\n","938/938 [==============================] - 9s 9ms/step - loss: 1613.6272\n","Epoch 58/100\n","938/938 [==============================] - 9s 10ms/step - loss: 2190.7909\n","Epoch 59/100\n","938/938 [==============================] - 9s 9ms/step - loss: 5169.7542\n","Epoch 60/100\n","938/938 [==============================] - 9s 9ms/step - loss: 3290.9536\n","Epoch 61/100\n","938/938 [==============================] - 9s 10ms/step - loss: 4140.8619\n","Epoch 62/100\n","938/938 [==============================] - 10s 10ms/step - loss: 5152.6415\n","Epoch 63/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3631.8071\n","Epoch 64/100\n","938/938 [==============================] - 10s 10ms/step - loss: 4774.1891\n","Epoch 65/100\n","938/938 [==============================] - 9s 10ms/step - loss: 4335.3712\n","Epoch 66/100\n","938/938 [==============================] - 10s 10ms/step - loss: 4844.2348\n","Epoch 67/100\n","938/938 [==============================] - 10s 10ms/step - loss: 3997.3263\n","Epoch 68/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3520.7430\n","Epoch 69/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5993.7245\n","Epoch 70/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3105.8593\n","Epoch 71/100\n","938/938 [==============================] - 9s 9ms/step - loss: 4879.0882\n","Epoch 72/100\n","938/938 [==============================] - 9s 9ms/step - loss: 5618.7522\n","Epoch 73/100\n","938/938 [==============================] - 9s 10ms/step - loss: 2601.8169\n","Epoch 74/100\n","938/938 [==============================] - 9s 9ms/step - loss: 5524.5564\n","Epoch 75/100\n","938/938 [==============================] - 9s 10ms/step - loss: 4978.1956\n","Epoch 76/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3841.3491\n","Epoch 77/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5337.5804\n","Epoch 78/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3671.7592\n","Epoch 79/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5580.5619\n","Epoch 80/100\n","938/938 [==============================] - 9s 9ms/step - loss: 4159.6768\n","Epoch 81/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3856.4255\n","Epoch 82/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5877.4157\n","Epoch 83/100\n","938/938 [==============================] - 10s 10ms/step - loss: 2768.6869\n","Epoch 84/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5378.4281\n","Epoch 85/100\n","938/938 [==============================] - 10s 11ms/step - loss: 3271.4507\n","Epoch 86/100\n","938/938 [==============================] - 10s 11ms/step - loss: 5229.9445\n","Epoch 87/100\n","938/938 [==============================] - 10s 11ms/step - loss: 4772.3011\n","Epoch 88/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3440.1726\n","Epoch 89/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5664.7925\n","Epoch 90/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3432.8571\n","Epoch 91/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5530.2659\n","Epoch 92/100\n","938/938 [==============================] - 10s 10ms/step - loss: 180182.9635\n","Epoch 93/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5682.6120\n","Epoch 94/100\n","938/938 [==============================] - 10s 11ms/step - loss: 4343.5895\n","Epoch 95/100\n","938/938 [==============================] - 10s 11ms/step - loss: 8730.0499\n","Epoch 96/100\n","938/938 [==============================] - 9s 10ms/step - loss: 5253.9788\n","Epoch 97/100\n","938/938 [==============================] - 9s 10ms/step - loss: 3463.5200\n","Epoch 98/100\n","938/938 [==============================] - 9s 9ms/step - loss: 5257.9100\n","Epoch 99/100\n","938/938 [==============================] - 10s 11ms/step - loss: 3894.4253\n","Epoch 100/100\n","938/938 [==============================] - 10s 10ms/step - loss: 4668.7059\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x31a28d610>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["vae.fit(x_train, x_train, epochs=num_steps, batch_size=batch_size)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["937.5"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["60000/64 # number of batches in one epoch"]},{"cell_type":"markdown","metadata":{"id":"EwmYUHDpr4N-"},"source":["# __Step 9: Generate a Manifold of Digits__\n","- Generate a manifold of digits by creating a latent space grid.\n","- Feed these grid values into the decoder to produce images."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"PXeObIz9svUe","outputId":"87b5e0d0-6d00-4a32-9d3f-98279038d7f0"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1000x1000 with 16 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["def generate_images(model, n_images):\n","    \"\"\"\n","    Generate and display new images using a trained VAE model.\n","    \n","    This function performs three main steps:\n","    1. Creates random points in the latent space\n","    2. Uses the decoder to convert these points into images\n","    3. Displays the generated images in a grid layout\n","    \n","    Args:\n","        model: The trained VAE model containing the decoder\n","        n_images: Number of images to generate and display\n","        \n","    Note:\n","        - Generated images will be displayed in a 4-column grid\n","        - Each image is 28x28 pixels (MNIST size) in grayscale\n","    \"\"\"\n","    # Step 1: Create random points in the latent space\n","    # - shape=(n_images, latent_dim) means we want n_images samples, each with latent_dim features\n","    # - tf.random.normal generates points from a standard normal distribution (mean=0, std=1)\n","    # - This mimics the distribution we trained our VAE to create\n","    random_latent_vectors = tf.random.normal(shape=(n_images, latent_dim))\n","    \n","    # Step 2: Use the decoder to convert these random points into images\n","    # - model.decoder converts points from latent space back to image space\n","    # - The output will be n_images of 28x28 pixels each\n","    generated_images = model.decoder(random_latent_vectors)\n","    \n","    # Convert TensorFlow tensor to NumPy array for plotting\n","    # - .numpy() converts TensorFlow tensor to a format matplotlib can use\n","    generated_images = generated_images.numpy()\n","\n","    # Calculate how many rows we need in our grid\n","    # - We want 4 images per row (hence dividing by 4)\n","    # - np.ceil rounds up to ensure we have enough rows\n","    # - int converts the float to integer for subplot\n","    n_rows = int(np.ceil(n_images / 4))\n","\n","    # Create a new figure for plotting with size 10x10 inches\n","    plt.figure(figsize=(10, 10))\n","    \n","    # Loop through each image and plot it\n","    for i in range(n_images):\n","        # Create a subplot in our grid\n","        # - n_rows: number of rows in grid\n","        # - 4: number of columns (we fixed this to 4)\n","        # - i + 1: current position (adding 1 because subplot counts from 1, not 0)\n","        plt.subplot(n_rows, 4, i + 1)\n","        \n","        # Display the image\n","        # - reshape(28, 28): convert flat array to 28x28 pixel image\n","        # - cmap='gray': use grayscale colormap (MNIST images are black and white)\n","        plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')\n","        \n","        # Remove axes for cleaner display\n","        plt.axis('off')\n","    \n","    # Show the entire grid of images\n","    plt.show()\n","\n","# Generate and display images\n","generate_images(vae, 16)"]},{"cell_type":"markdown","metadata":{"id":"zNc7jyqlGx5u"},"source":["# __Conclusion__\n","\n","In this demonstration, we have successfully implemented and trained a Variational Autoencoder (VAE) using TensorFlow to generate images based on the MNIST dataset. The process encompassed several critical steps, from importing necessary libraries to training the model, and finally, generating a manifold of digits."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"iitk_gen_ai_course","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":0}
